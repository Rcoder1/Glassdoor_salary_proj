{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, StaleElementReferenceException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "\n",
    "def get_jobs(keyword, num_jobs, locId, verbose):\n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "\n",
    "    # Initializing the webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    # Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n",
    "    # options.add_argument('headless')\n",
    "\n",
    "    # Change the path to where chromedriver is in your home folder.\n",
    "    driver = webdriver.Chrome(executable_path=\"chromedriver\", options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    # url\n",
    "    url = \"https://www.glassdoor.com/Job/jobs.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=\" + \\\n",
    "        keyword+\"&sc.keyword=\"+keyword+\"&locT=C&locId=\"+locId+\"&jobType=\"\n",
    "    \n",
    "    #url = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=\"' + keyword + '\"&locT=C&locId=1147401&locKeyword=San%20Francisco,%20CA&jobType=all&fromAge=-1&minSalary=0&includeNoSalaryJobs=true&radius=100&cityId=-1&minRating=0.0&industryId=-1&sgocId=-1&seniorityType=all&companyId=-1&employerSizes=0&applicationType=0&remoteWorkType=0'\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH,\n",
    "        './/li[@class=\"react-job-listing css-bkasv9 eigr9kq0\"]').click()\n",
    "\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        # clicking to the X.\n",
    "        driver.find_element(By.CSS_SELECTOR,'[alt=\"Close\"]').click()\n",
    "        print(' x out worked')\n",
    "    except NoSuchElementException:\n",
    "        print(' x out failed')\n",
    "        pass\n",
    "\n",
    "    # If true, should be still looking for new jobs.\n",
    "    while len(jobs) < num_jobs:\n",
    "        try:\n",
    "            stale = False\n",
    "            # Going through each job in this page\n",
    "            # jl for Job Listing. These are the buttons we're going to click.\n",
    "            job_buttons = driver.find_elements(By.XPATH,\n",
    "                \".//*[contains(@class,'react-job-listing')]\")\n",
    "            print(len(job_buttons))\n",
    "            for job_button in job_buttons:\n",
    "\n",
    "                print(\"Progress: {}\".format(\n",
    "                    \"\" + str(len(jobs)) + \"/\" + str(num_jobs)))\n",
    "                if len(jobs) >= num_jobs:\n",
    "                    break\n",
    "                try:\n",
    "                    job_button.click()  # You might\n",
    "                    print('clicked')\n",
    "                except StaleElementReferenceException:\n",
    "                    print(\"Stale element found\")\n",
    "                    stale = True\n",
    "                    break\n",
    "\n",
    "                sleep_time = random.randint(1,6)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                collected_successfully = False\n",
    "\n",
    "                while not collected_successfully:\n",
    "                    try:\n",
    "                        company_name = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"css-xuk5ye e1tk4kwz5\"]').text.split(\"\\n\")[0]\n",
    "                        location = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"css-56kyx5 e1tk4kwz1\"]').text\n",
    "                        job_title = driver.find_element(By.XPATH,\n",
    "                            './/div[contains(@class, \"css-1j389vi e1tk4kwz2\")]').text\n",
    "                        job_description = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"jobDescriptionContent desc\"]').text\n",
    "                        collected_successfully = True\n",
    "                        print(\"Collected successfully\")\n",
    "                    except Exception as e:\n",
    "                        print(\"Exception\" + str(e))\n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "\n",
    "                try:\n",
    "                    salary_estimate = driver.find_element(By.XPATH,\n",
    "                        './/span[@class=\"css-1hbqxax e1wijj240\"]').text\n",
    "                    print(\"Collected successfully salary\")\n",
    "                except NoSuchElementException:\n",
    "                    print(\"No such element exception\")\n",
    "                    salary_estimate = -1  # You need to set a \"not found value. It's important.\"\n",
    "\n",
    "                try:\n",
    "                    rating = driver.find_element(By.XPATH,\n",
    "                        './/span[@class=\"css-1m5m32b e1tk4kwz4\"]').text\n",
    "                    print(\"Collected successfully rating\")\n",
    "                except NoSuchElementException:\n",
    "                    print (\"No rating\")\n",
    "                    rating = -1  # You need to set a \"not found value. It's important.\"\n",
    "\n",
    "                # Printing for debugging\n",
    "                if verbose:\n",
    "                    print(\"Job Title: {}\".format(job_title))\n",
    "                    print(\"Salary Estimate: {}\".format(salary_estimate))\n",
    "                    print(\"Job Description: {}\".format(job_description[:500]))\n",
    "                    print(\"Rating: {}\".format(rating))\n",
    "                    print(\"Company Name: {}\".format(company_name))\n",
    "                    print(\"Location: {}\".format(location))\n",
    "\n",
    "                # Going to the Company tab...\n",
    "                # clicking on this:\n",
    "                # <div class=\"tab\" data-tab-type=\"overview\"><span>Company</span></div>\n",
    "                try:\n",
    "    #                 driver.find_element(By.XPATH,\n",
    "    #                     './/div[@data-test=\"overview\"]').click()\n",
    "\n",
    "    #                 time.sleep(2)\n",
    "\n",
    "                    try:\n",
    "                        size = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"d-flex justify-content-start css-daag8o e1pvx6aw2\"]//span[text()=\"Size\"]//following-sibling::*').text\n",
    "                        print(\"Collected successfully size\")\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"No size\")\n",
    "                        size = -1\n",
    "\n",
    "                    try:\n",
    "                        founded = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"d-flex justify-content-start css-daag8o e1pvx6aw2\"]//span[text()=\"Founded\"]//following-sibling::*').text\n",
    "                        print(\"Collected successfully FOUNDED\")\n",
    "                    except NoSuchElementException:\n",
    "                        founded = -1\n",
    "\n",
    "                    try:\n",
    "                        type_of_ownership = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"d-flex justify-content-start css-daag8o e1pvx6aw2\"]//span[text()=\"Type\"]//following-sibling::*').text\n",
    "                        print(\"Collected successfully TYPE\")\n",
    "                    except NoSuchElementException:\n",
    "                        type_of_ownership = -1\n",
    "\n",
    "                    try:\n",
    "                        industry = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"d-flex justify-content-start css-daag8o e1pvx6aw2\"]//span[text()=\"Industry\"]//following-sibling::*').text\n",
    "                        print(\"Collected successfully industry\")\n",
    "                    except NoSuchElementException:\n",
    "                        industry = -1\n",
    "\n",
    "                    try:\n",
    "                        sector = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"d-flex justify-content-start css-daag8o e1pvx6aw2\"]//span[text()=\"Sector\"]//following-sibling::*').text\n",
    "                    except NoSuchElementException:\n",
    "                        sector = -1\n",
    "\n",
    "                    try:\n",
    "                        revenue = driver.find_element(By.XPATH,\n",
    "                            './/div[@class=\"d-flex justify-content-start css-daag8o e1pvx6aw2\"]//span[text()=\"Revenue\"]//following-sibling::*').text\n",
    "                    except NoSuchElementException:\n",
    "                        revenue = -1\n",
    "\n",
    "                # Rarely, some job postings do not have the \"Company\" tab.\n",
    "                except NoSuchElementException:\n",
    "                    print(\"No company\")\n",
    "                    size = -1\n",
    "                    founded = -1\n",
    "                    type_of_ownership = -1\n",
    "                    industry = -1\n",
    "                    sector = -1\n",
    "                    revenue = -1\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"Size: {}\".format(size))\n",
    "                    print(\"Founded: {}\".format(founded))\n",
    "                    print(\"Type of Ownership: {}\".format(type_of_ownership))\n",
    "                    print(\"Industry: {}\".format(industry))\n",
    "                    print(\"Sector: {}\".format(sector))\n",
    "                    print(\"Revenue: {}\".format(revenue))\n",
    "                    print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "                jobs.append({\"Job Title\": job_title,\n",
    "                             \"Salary Estimate\": salary_estimate,\n",
    "                             \"Job Description\": job_description,\n",
    "                             \"Rating\": rating,\n",
    "                             \"Company Name\": company_name,\n",
    "                             \"Location\": location,\n",
    "                             \"Size\": size,\n",
    "                             \"Founded\": founded,\n",
    "                             \"Type of ownership\": type_of_ownership,\n",
    "                             \"Industry\": industry,\n",
    "                             \"Sector\": sector,\n",
    "                             \"Revenue\": revenue})\n",
    "                # add job to jobs\n",
    "\n",
    "            # Clicking on the \"next page\" button\n",
    "            try:\n",
    "                if not stale:\n",
    "                    driver.find_element(By.XPATH,\n",
    "                        './/button[@class=\"nextButton css-1hq9k8 e13qs2071\"]').click()\n",
    "                    print(\"Going to the next page\")\n",
    "                else:\n",
    "                    print(\"Stale, not going to next page\")\n",
    "            except NoSuchElementException:\n",
    "                print(\"Scraping terminated before reaching target number of jobs. Needed {}, got {}.\".format(\n",
    "                    num_jobs, len(jobs)))\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            traceback.print_tb(e.__traceback__)\n",
    "            print(\"Had an exception, retrieved {}.\".format(len(jobs)))\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "            \n",
    "\n",
    "    # This line converts the dictionary object into a pandas DataFrame.\n",
    "    return pd.DataFrame(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#San Francisco, CA\n",
    "# 1147401 is the location ID of SF\n",
    "df_SF = get_jobs('data scientist', 100,'1147401',False)\n",
    "df_SF.to_csv('df_SF.csv',index = False)\n",
    "# to check numvber of duplicate job postings\n",
    "df_SF.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98a7e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to check the column names\n",
    "df_SF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC, NY\n",
    "df_NYC= get_jobs('data scientist', 100, '1132348',False)\n",
    "df_NYC.to_csv('df_NYC.csv',index = False)\n",
    "df_NYC.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raleigh, NC\n",
    "df_Raleigh= get_jobs('data scientist', 100, '1138960',False)\n",
    "df_Raleigh.to_csv('df_Raleigh.csv',index = False)\n",
    "df_Raleigh.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd121816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Philadelphia, PA\n",
    "df_Phili= get_jobs('data scientist', 100, '1152672', False)\n",
    "df_Phili.to_csv('df_Philadelphia.csv',index = False)\n",
    "df_Phili.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65582a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dallas, TX\n",
    "df_Dallas= get_jobs('data scientist', 100,'1139977',False)\n",
    "df_Dallas.to_csv('df_Dallas.csv',index = False)\n",
    "df_Dallas.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miami, Fl\n",
    "df_Miami = get_jobs('data scientist', 100,'1154170',False)\n",
    "df_Miami.to_csv('df_Miami.csv',index = False)\n",
    "df_Miami.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atlanta, Ga\n",
    "df_Atlanta = get_jobs('data scientist', 100,'1155583',False)\n",
    "df_Atlanta.to_csv('df_Atlanta.csv',index = False)\n",
    "df_Atlanta.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef18166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detroit, MI\n",
    "df_Detroit = get_jobs('data scientist', 100,'1134644',False)\n",
    "df_Detroit.to_csv('df_Detroit.csv',index = False)\n",
    "df_Detroit.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd7c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleveland, OH\n",
    "df_Cleveland = get_jobs('data scientist', 100,'1145778',False)\n",
    "df_Cleveland.to_csv('df_Cleveland.csv',index = False)\n",
    "df_Cleveland.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denver, CO\n",
    "df_Denver = get_jobs('data scientist', 100,'1148170',False)\n",
    "df_Denver.to_csv('df_Denver.csv',index = False)\n",
    "df_Denver.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orlando, FL\n",
    "df_Orlando = get_jobs('data scientist', 100,'1154247',False)\n",
    "df_Orlando.to_csv('df_Orlando.csv',index = False)\n",
    "df_Orlando.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bdc097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# St.louis, Mo\n",
    "df_St_Louis = get_jobs('data scientist', 100,'1131270',False)\n",
    "df_St_Louis.to_csv('df_St_Louis.csv',index = False)\n",
    "df_St_Louis.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8372fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charlotte, Nc\n",
    "df_Charlotte = get_jobs('data scientist', 100,'1138644',False)\n",
    "df_Charlotte.to_csv('df_Charlotte.csv',index = False)\n",
    "df_Charlotte.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salt Lake City, Ut\n",
    "df_Salt_Lake_City = get_jobs('data scientist', 100,'1128289',False)\n",
    "df_Salt_Lake_City.to_csv('df_Salt_Lake_City.csv',index = False)\n",
    "df_Salt_Lake_City.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columbus, Oh\n",
    "df_Columbus = get_jobs('data scientist', 100,'1145845',False)\n",
    "df_Columbus.to_csv('df_Columbus.csv',index = False)\n",
    "df_Columbus.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las Vegas, Nv\n",
    "df_Las_Vegas = get_jobs('data scientist', 100,'1149603',False)\n",
    "df_Las_Vegas.to_csv('df_Las_Vegas.csv',index = False)\n",
    "df_Las_Vegas.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac28333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kansas City, Mo\n",
    "df_Kansas_City = get_jobs('data scientist', 100,'1131040',False)\n",
    "df_Kansas_City.to_csv('df_Kansas_City.csv',index = False)\n",
    "df_Kansas_City.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indianapolis, In\n",
    "df_Indianapolis = get_jobs('data scientist', 100,'1145013',False)\n",
    "df_Indianapolis.to_csv('df_Indianapolis.csv',index = False)\n",
    "df_Indianapolis.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d271b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cincinnati, Oh\n",
    "df_Cincinnati = get_jobs('data scientist', 100,'1145705',False)\n",
    "df_Cincinnati.to_csv('df_Cincinnati.csv',index = False)\n",
    "df_Cincinnati.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ba0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoenix, Az\n",
    "df_Phoenix = get_jobs('data scientist', 100,'1133904',False)\n",
    "df_Phoenix.to_csv('df_Phoenix.csv',index = False)\n",
    "df_Phoenix.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portland, OR\n",
    "df_Portland = get_jobs('data scientist', 100,'1151614',False)\n",
    "df_Portland.to_csv('df_Portland.csv',index = False)\n",
    "df_Portland.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a18c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Houston, Tx\n",
    "df_Houston = get_jobs('data scientist', 100,'1140171',False)\n",
    "df_Houston.to_csv('df_Houston.csv',index = False)\n",
    "df_Houston.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226573d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seattle, WA\n",
    "df_Seattle = get_jobs('data scientist', 100,'1150505',False)\n",
    "df_Seattle.to_csv('df_Seattle.csv',index = False)\n",
    "df_Seattle.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ebc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Austin, Tx\n",
    "df_Austin = get_jobs('data scientist', 100,'1139761',False)\n",
    "df_Austin.to_csv('df_Austin.csv',index = False)\n",
    "df_Austin.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boston, MA\n",
    "df_Boston = get_jobs('data scientist', 100,'1154532',False)\n",
    "df_Boston.to_csv('df_Boston.csv',index = False)\n",
    "df_Boston.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07664e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Washington, DC\n",
    "df_Washington_DC = get_jobs('data scientist', 100,'1138213',False)\n",
    "df_Washington_DC.to_csv('df_Washington_DC.csv',index = False)\n",
    "df_Washington_DC.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591952fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arlington, VA\n",
    "df_Arlington = get_jobs('data scientist', 100,'1130337',False)\n",
    "df_Arlington.to_csv('df_Arlington.csv',index = False)\n",
    "df_Arlington.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b82082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los Angeles, CA\n",
    "df_LA = get_jobs('data scientist', 100,'1146821',False)\n",
    "df_LA.to_csv('df_LA.csv',index = False)\n",
    "df_LA.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chicago, IL\n",
    "df_Chicago = get_jobs('data scientist', 100,'1128808',False)\n",
    "df_Chicago.to_csv('df_Chicago.csv',index = False)\n",
    "df_Chicago.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minneapolis, MN\n",
    "df_Minneapolis = get_jobs('data scientist', 100,'1142551',False)\n",
    "df_Minneapolis.to_csv('df_Minneapolis.csv',index = False)\n",
    "df_Minneapolis.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baltimore, MD\n",
    "df_Baltimore = get_jobs('data scientist', 100,'1153527',False)\n",
    "df_Baltimore.to_csv('df_Baltimore.csv',index = False)\n",
    "df_Baltimore.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d778c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# San Diego, CA\n",
    "df_San_Diego = get_jobs('data scientist', 100,'1147311',False)\n",
    "df_San_Diego.to_csv('df_San_Diego.csv',index = False)\n",
    "df_San_Diego.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Irvine, CA\n",
    "df_Irvine_CA = get_jobs('data scientist', 100,'1146798',False)\n",
    "df_Irvine_CA.to_csv('df_Irvine_CA.csv',index = False)\n",
    "df_Irvine_CA.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampa, FL\n",
    "df_Tampa = get_jobs('data scientist', 100,'1154429',False)\n",
    "df_Tampa.to_csv('df_Tampa.csv',index = False)\n",
    "df_Tampa.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de6d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nashville, TN\n",
    "df_Nashville= get_jobs('data scientist', 58,'1144541',False)\n",
    "df_Nashville.to_csv('df_Nashville.csv',index = False)\n",
    "df_Nashville.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santa Barbara\n",
    "df_Santa_Barbara= get_jobs('data scientist', 100,'1147473',False)\n",
    "df_Santa_Barbara.to_csv('df_Santa_Barbara.csv',index = False)\n",
    "df_Santa_Barbara.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oakland, CA\n",
    "df_Oakland= get_jobs('data scientist', 60,'1147380',False)\n",
    "df_Oakland.to_csv('df_Oakland.csv',index = False)\n",
    "df_Oakland.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fremont, CA\n",
    "df_Fremont= get_jobs('data scientist', 100,'1147355',False)\n",
    "df_Fremont.to_csv('df_Fremont.csv',index = False)\n",
    "df_Fremont.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Honolulu, HI \n",
    "df_Honolulu= get_jobs('data scientist', 60,'1140656',False)\n",
    "df_Honolulu.to_csv('df_Honolulu.csv',index = False)\n",
    "df_Honolulu.duplicated().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
